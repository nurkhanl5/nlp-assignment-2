{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10124226,"sourceType":"datasetVersion","datasetId":6247480},{"sourceId":10124299,"sourceType":"datasetVersion","datasetId":6247532},{"sourceId":10131972,"sourceType":"datasetVersion","datasetId":6253156},{"sourceId":10131983,"sourceType":"datasetVersion","datasetId":6253166}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q -U bitsandbytes\n!!pip install -q -U accelerate\n!pip install peft","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.idle":"2024-12-09T18:25:04.362482Z","shell.execute_reply.started":"2024-12-09T18:24:35.723034Z","shell.execute_reply":"2024-12-09T18:25:04.361155Z"}},"outputs":[{"name":"stdout","text":"Installing collected packages: peft\nSuccessfully installed peft-0.14.0\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import pandas as pd","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T18:25:24.528473Z","iopub.execute_input":"2024-12-09T18:25:24.529079Z","iopub.status.idle":"2024-12-09T18:25:24.533442Z","shell.execute_reply.started":"2024-12-09T18:25:24.529048Z","shell.execute_reply":"2024-12-09T18:25:24.532375Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"import os\nimport random\nimport functools\nimport csv\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom sklearn.metrics import f1_score\nfrom skmultilearn.model_selection import iterative_train_test_split\nfrom datasets import Dataset, DatasetDict\nfrom peft import (\n    LoraConfig,\n    prepare_model_for_kbit_training,\n    get_peft_model\n)\nfrom transformers import (\n    AutoModelForSequenceClassification,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    TrainingArguments,\n    Trainer\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T18:25:04.364383Z","iopub.execute_input":"2024-12-09T18:25:04.364705Z","iopub.status.idle":"2024-12-09T18:25:24.527208Z","shell.execute_reply.started":"2024-12-09T18:25:04.364673Z","shell.execute_reply":"2024-12-09T18:25:24.526284Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"annotations = pd.read_csv('/kaggle/input/subtaskqkk/subtask-1-annotations.txt', sep=\"\\t\", header=None,\n                          names=[\"article_id\", \"entity\", \"start_offset\", \"end_offset\", \"main_role\", \"fine_grained_roles\"],\n                          on_bad_lines=\"skip\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T18:25:48.792767Z","iopub.execute_input":"2024-12-09T18:25:48.793657Z","iopub.status.idle":"2024-12-09T18:25:48.812278Z","shell.execute_reply.started":"2024-12-09T18:25:48.793619Z","shell.execute_reply":"2024-12-09T18:25:48.811564Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"annotations.to_csv('annotations.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T18:26:14.391929Z","iopub.execute_input":"2024-12-09T18:26:14.392315Z","iopub.status.idle":"2024-12-09T18:26:14.401341Z","shell.execute_reply.started":"2024-12-09T18:26:14.392273Z","shell.execute_reply":"2024-12-09T18:26:14.400584Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"import os\nimport csv\nimport random\nimport numpy as np\nfrom datasets import Dataset, DatasetDict\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.utils import shuffle\n\n# Define paths\ndata_file = 'annotations.csv'  # Your dataset\ntext_path = '/kaggle/input/subtask1/raw-documents/raw-documents/'  # Folder containing the article_id text files\n\n# Label mappings\nfine_grained_roles = [\n    \"Guardian\", \"Martyr\", \"Peacemaker\", \"Rebel\", \"Underdog\", \"Virtuous\", \"Instigator\",\n    \"Conspirator\", \"Tyrant\", \"Foreign Adversary\", \"Traitor\", \"Spy\", \"Saboteur\",\n    \"Corrupt\", \"Incompetent\", \"Terrorist\", \"Deceiver\", \"Bigot\", \"Forgotten\",\n    \"Exploited\", \"Victim\", \"Scapegoat\"\n]\nlabel_mapping = {label: i for i, label in enumerate(fine_grained_roles)}\n\n# Read data\ndata = []\nwith open(data_file, newline='', encoding='utf-8') as csvfile:\n    reader = csv.DictReader(csvfile)\n    for row in reader:\n        # Read the text content from the file\n        file_path = os.path.join(text_path, row['article_id'])\n        if not file_path.endswith('EN_UA_$.txt'):\n            with open(file_path, 'r', encoding='utf-8') as f:\n                text = f.read()\n    \n            # Create a one-hot encoded label for fine_grained_roles\n            labels = [0] * len(fine_grained_roles)\n            for role in row['fine_grained_roles'].split(','):  # Assume comma-separated labels\n                role = role.strip()\n                if role in label_mapping:\n                    labels[label_mapping[role]] = 1\n    \n            data.append({\n                'text': text,\n                'labels': labels\n            })\n\n# Shuffle the data\nrandom.shuffle(data)\n\n# Extract texts and labels\ntexts = [item['text'] for item in data]\nlabels = np.array([item['labels'] for item in data], dtype=int)\n\n# Compute label weights (optional)\nlabel_weights = 1 - labels.sum(axis=0) / labels.sum()\n\n# Train/test split (stratified for multilabel)\nrow_ids = np.arange(len(labels))\ntrain_idx, val_idx = train_test_split(row_ids, test_size=0.1, stratify=labels.sum(axis=1))\nx_train = [texts[i] for i in train_idx]\nx_val = [texts[i] for i in val_idx]\ny_train = labels[train_idx]\ny_val = labels[val_idx]\n\n# Create Hugging Face Dataset\nds = DatasetDict({\n    'train': Dataset.from_dict({'text': x_train, 'labels': y_train.tolist()}),\n    'val': Dataset.from_dict({'text': x_val, 'labels': y_val.tolist()})\n})\n\n# Model name (replace with your own model)\nmodel_name = 'mistralai/Mistral-7B-v0.1'\n\n# Print Dataset Details\nprint(ds)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T18:46:44.118533Z","iopub.execute_input":"2024-12-09T18:46:44.118905Z","iopub.status.idle":"2024-12-09T18:46:44.341810Z","shell.execute_reply.started":"2024-12-09T18:46:44.118873Z","shell.execute_reply":"2024-12-09T18:46:44.340445Z"}},"outputs":[{"name":"stdout","text":"DatasetDict({\n    train: Dataset({\n        features: ['text', 'labels'],\n        num_rows: 349\n    })\n    val: Dataset({\n        features: ['text', 'labels'],\n        num_rows: 39\n    })\n})\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# set random seed\nrandom.seed(0)\n\n# load data\nwith open('/kaggle/working/annotations.csv', newline='') as csvfile:\n    data = list(csv.reader(csvfile, delimiter=','))\n    header_row = data.pop(0)\n\n# shuffle data\nrandom.shuffle(data)\n\n# reshape\nidx, text, labels = list(zip(*[(int(row[0]), f'Title: {row[1].strip()}\\n\\nAbstract: {row[2].strip()}', row[3:]) for row in data]))\nlabels = np.array(labels, dtype=int)\n\n# create label weights\nlabel_weights = 1 - labels.sum(axis=0) / labels.sum()\n\n# stratified train test split for multilabel ds\nrow_ids = np.arange(len(labels))\ntrain_ids = row_ids\n#train_idx, y_train, val_idx, y_val = iterative_train_test_split(row_ids[:,np.newaxis], labels, test_size = 0.1)\nx_train = [text[i] for i in train_idx.flatten()]\n\n# create hf dataset\nds = DatasetDict({\n    'train': Dataset.from_dict({'text': x_train, 'labels': y_train}),\n    'val': Dataset.from_dict({'text': x_val, 'labels': y_val})\n})\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T18:47:17.274574Z","iopub.execute_input":"2024-12-09T18:47:17.275201Z","iopub.status.idle":"2024-12-09T18:47:17.285386Z","shell.execute_reply.started":"2024-12-09T18:47:17.275164Z","shell.execute_reply":"2024-12-09T18:47:17.284456Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"#model_name = \"meta-llama/Llama-3.1-7b\"\naccess_token = \"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\"\n\n'''tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=access_token)\nmodel = AutoModelForCausalLM.from_pretrained(model_name, use_auth_token=access_token)\n'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T18:51:34.438643Z","iopub.execute_input":"2024-12-09T18:51:34.439446Z","iopub.status.idle":"2024-12-09T18:51:34.445548Z","shell.execute_reply.started":"2024-12-09T18:51:34.439409Z","shell.execute_reply":"2024-12-09T18:51:34.444607Z"}},"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"'tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=access_token)\\nmodel = AutoModelForCausalLM.from_pretrained(model_name, use_auth_token=access_token)\\n'"},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"\n# preprocess dataset with tokenizer\ndef tokenize_examples(examples, tokenizer):\n    tokenized_inputs = tokenizer(examples['text'])\n    tokenized_inputs['labels'] = examples['labels']\n    return tokenized_inputs\n\ntokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=access_token)\ntokenizer.pad_token = tokenizer.eos_token\ntokenized_ds = ds.map(functools.partial(tokenize_examples, tokenizer=tokenizer), batched=True)\ntokenized_ds = tokenized_ds.with_format('torch')\n\n# qunatization config\nquantization_config = BitsAndBytesConfig(\n    load_in_4bit = True, # enable 4-bit quantization\n    bnb_4bit_quant_type = 'nf4', # information theoretically optimal dtype for normally distributed weights\n    bnb_4bit_use_double_quant = True, # quantize quantized weights //insert xzibit meme\n    bnb_4bit_compute_dtype = torch.bfloat16 # optimized fp format for ML\n)\n\n# lora config\nlora_config = LoraConfig(\n    r = 16, # the dimension of the low-rank matrices\n    lora_alpha = 8, # scaling factor for LoRA activations vs pre-trained weight activations\n    target_modules = ['q_proj', 'k_proj', 'v_proj', 'o_proj'],\n    lora_dropout = 0.05, # dropout probability of the LoRA layers\n    bias = 'none', # wether to train bias weights, set to 'none' for attention layers\n    task_type = 'SEQ_CLS'\n)\n\n# load model\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    model_name, use_auth_token=access_token,\n    quantization_config=quantization_config,\n    num_labels=labels.shape[1]\n)\nmodel = prepare_model_for_kbit_training(model)\nmodel = get_peft_model(model, lora_config)\nmodel.config.pad_token_id = tokenizer.pad_token_id\n\n# define training args\ntraining_args = TrainingArguments(\n    output_dir = 'multilabel_classification',\n    learning_rate = 1e-4,\n    per_device_train_batch_size = 8, # tested with 16gb gpu ram\n    per_device_eval_batch_size = 8,\n    num_train_epochs = 10,\n    weight_decay = 0.01,\n    evaluation_strategy = 'epoch',\n    save_strategy = 'epoch',\n    load_best_model_at_end = True\n)\n\n# train\nclass CustomTrainer(Trainer):\n    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n        \"\"\"\n        Custom loss computation.\n        \"\"\"\n        # Extract labels from inputs\n        labels = inputs.pop(\"labels\")\n        # Forward pass\n        outputs = model(**inputs)\n        logits = outputs.logits\n        # Compute custom loss\n        loss_fct = nn.BCEWithLogitsLoss(pos_weight=self.label_weights)\n        loss = loss_fct(logits, labels.float())\n        return (loss, outputs) if return_outputs else loss\n\ntrainer = CustomTrainer(\n    model = model,\n    args = training_args,\n    train_dataset = tokenized_ds['train'],\n    eval_dataset = tokenized_ds['val'],\n    tokenizer = tokenizer,\n    data_collator = functools.partial(collate_fn, tokenizer=tokenizer),\n    compute_metrics = compute_metrics,\n    label_weights = torch.tensor(label_weights, device=model.device)\n)\n\ntrainer.train()\n\n# save model\npeft_model_id = 'multilabel_mistral'\ntrainer.model.save_pretrained(peft_model_id)\ntokenizer.save_pretrained(peft_model_id)\n\n# load model\npeft_model_id = 'multilabel_mistral'\nmodel = AutoModelForSequenceClassification.from_pretrained(peft_model_id)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import Trainer\nimport torch.nn as nn\n\nclass CustomTrainer(Trainer):\n    def __init__(self, label_weights=None, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.label_weights = label_weights\n\n    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n        \"\"\"\n        Custom loss computation.\n        \"\"\"\n        # Extract labels from inputs\n        labels = inputs.pop(\"labels\")\n        \n        # Forward pass\n        outputs = model(**inputs)\n        logits = outputs.logits\n\n        # Compute custom loss with label weights\n        loss_fct = nn.BCEWithLogitsLoss(pos_weight=self.label_weights)\n        loss = loss_fct(logits, labels.float())\n\n        # Return loss and optionally outputs\n        return (loss, outputs) if return_outputs else loss\n\ntrainer = CustomTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_ds['train'],\n    eval_dataset=tokenized_ds['val'],\n    tokenizer=tokenizer,\n    data_collator=functools.partial(collate_fn, tokenizer=tokenizer),\n    compute_metrics=compute_metrics,\n    label_weights=torch.tensor(label_weights, device=model.device)  # Explicitly pass label weights\n)\n\n\ntrainer.train()\n\n\npeft_model_id = 'multilabel_mistral'\ntrainer.model.save_pretrained(peft_model_id)\ntokenizer.save_pretrained(peft_model_id)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}